{"version":3,"sources":["fakes012000.png","attempt_2_final.png","App.js","serviceWorker.js","firebase.js","pages/Gans.js","pages/Gitpages.js","pages/Classifier.js","pages/Firebase.js","pages/Coral.js","pages/Dsm.js","index.js","react_logo.jpg","git_sh.png","pics/dsm.jpg"],"names":["module","exports","App","className","exact","activeClassName","style","color","to","Boolean","window","location","hostname","match","firebase","initializeApp","apiKey","authDomain","databaseURL","projectId","storageBucket","messagingSenderId","Gans","src","result","alt","height","width","id","href","target","result2","noValidate","autoComplete","TextField","multiline","rowsMax","varient","label","Gitpages","react_logo","class","language","dracula","git_sh","Classifier","Coral","Dsm","car","ReactDOM","render","StrictMode","path","component","Firebase","document","getElementById","navigator","serviceWorker","ready","then","registration","unregister","catch","error","console","message"],"mappings":"6HAAAA,EAAOC,QAAU,IAA0B,yC,mBCA3CD,EAAOC,QAAU,IAA0B,6C,iGCmE5BC,MA/Df,WACE,OACE,yBAAKC,UAAU,OACb,4BAAQA,UAAU,cAEhB,2BACE,qDAAmC,6BACnC,6DAEF,yBAAKA,UAAU,SACb,2BAAG,2FACQ,0BAAMA,UAAU,SAAhB,kBACX,2BAAG,wFACH,2BAAG,8BAAM,0BAAMA,UAAU,SAAhB,YAAN,YACH,2BAAG,wFACG,0BAAMA,UAAU,SAAhB,YACN,2BAAG,wFACH,2BAAG,wFACH,2BAAG,wFACG,0BAAMA,UAAU,SAAhB,aAEN,2BAAG,uFACH,2BAAG,uFAEH,2BAAG,2FACQ,0BAAMA,UAAU,YAAW,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,SAAnE,iCACtC,2BAAG,wFACH,2BAAG,wFACH,2BAAG,wFACG,0BAAML,UAAU,YAAW,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,UAAnE,wCACjC,2BAAG,wFACH,2BAAG,8BAAM,0BAAML,UAAU,YAAhB,YAAN,YACH,2BAAG,wFACG,0BAAMA,UAAU,YAAW,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,UAAnE,qCACjC,2BAAG,wFACH,2BAAG,wFACH,2BAAG,wFACG,0BAAML,UAAU,YAAW,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,aAAnE,qCAC3C,2BAAG,wFACO,2BAAG,wFACH,2BAAG,wFACG,0BAAML,UAAU,YAAW,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,SAAnE,0CAEjC,2BAAG,uFACH,2BAAG,uFAEH,2BAAG,2FACQ,0BAAML,UAAU,QAAO,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,WAAYC,GAAG,QAAtE,0BAClC,2BAAG,wFACH,2BAAG,8BAAM,0BAAML,UAAU,QAAhB,UAAN,gBACH,2BAAG,wFACG,0BAAMA,UAAU,QAAhB,0BACN,2BAAG,wFACH,2BAAG,wFACH,2BAAG,wFACG,0BAAMA,UAAU,QAAhB,8B,OC/CIM,QACW,cAA7BC,OAAOC,SAASC,UAEe,UAA7BF,OAAOC,SAASC,UAEhBF,OAAOC,SAASC,SAASC,MACvB,2D,qBCRNC,IAASC,cARM,CACXC,OAAQ,2CACRC,WAAY,qCACZC,YAAa,6CACbC,UAAW,qBACXC,cAAe,iCACfC,kBAAmB,iBAGRP,EAAf,EAAeA,I,yDC6FAQ,MA7Ff,WACE,OACE,yBAAKnB,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,YAE1B,oEAEA,yBAAKe,IAAKC,IAAQC,IAAI,aAAaC,OAAO,MAAMC,MAAM,QAEtD,uBAAGxB,UAAU,YAAW,8BAAM,0BAAMA,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,iBAAT,aAA7B,OAAmF,0BAAML,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,YAAT,kBAA1G,OAAgK,0BAAML,UAAU,SAAQ,kBAAC,WAAD,CAAMK,GAAG,cAAT,YAAxL,OAA0O,0BAAML,UAAU,SAAQ,kBAAC,WAAD,CAAMK,GAAG,aAAT,qBAAlQ,OAA4T,0BAAML,UAAU,SAAQ,kBAAC,WAAD,CAAMK,GAAG,gBAAT,YAApV,MAAuY,0BAAML,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,iBAAT,gBAEtb,uBAAGoB,GAAG,WAAWzB,UAAU,WAAU,6CACrC,uBAAGA,UAAU,QAAb,4dAEA,uBAAGyB,GAAG,MAAMzB,UAAU,WAAU,kDAChC,wBAAIA,UAAU,aACV,iFACA,kEACA,qEAGJ,uBAAGyB,GAAG,QAAQzB,UAAU,WAAU,mEAClC,uBAAGA,UAAU,QAAb,sJAAuK,uBAAG0B,KAAK,sEAAsEC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAArQ,oOAA4gB,uBAAG0B,KAAK,oFAAoFC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAAxnB,gEAA2tB,wCAA3tB,qZAEA,uBAAGA,UAAU,QAAb,yGAA0H,uBAAG0B,KAAK,wDAAwDC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAA1M,oXAEA,uBAAGyB,GAAG,OAAOzB,UAAU,WAAU,qDACjC,uBAAGA,UAAU,QAAb,mIAAoJ,uBAAG0B,KAAK,yDAAyDC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAArO,6bAEA,uBAAGyB,GAAG,UAAUzB,UAAU,WAAU,4CAEpC,uBAAGA,UAAU,QAAb,qHACA,2BAAOA,UAAU,UAAUwB,MAAM,OACjC,4BACE,uCACA,uCACA,4CACA,0CACA,wCACA,0CAEF,4BACE,iCACA,kDACA,oCACA,yCACA,0CACA,8CAEF,4BACE,iCACA,iDAAsB,uBAAGE,KAAK,wDAAwDC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAAtG,KACA,oCACA,yCACA,0CACA,8CAEF,4BACE,iCACA,iDAAsB,uBAAG0B,KAAK,wDAAwDC,OAAO,UAAS,0BAAM3B,UAAU,QAAhB,SAAtG,KACA,sCACA,yCACA,mCACA,+CAIF,uBAAGA,UAAU,QAAb,oEAAqF,6CAArF,yBACA,yBAAKoB,IAAKC,IAAQC,IAAI,aAAaC,OAAO,MAAMC,MAAM,QAEtD,uBAAGxB,UAAU,QAAb,ggBAAihB,8BAEjhB,uBAAGA,UAAU,QAAb,oEAAqF,6CAArF,yBACA,yBAAKoB,IAAKQ,IAASN,IAAI,aAAaC,OAAO,MAAMC,MAAM,QAEvD,uBAAGxB,UAAU,QAAb,kpBAAmqB,8BAEnqB,uBAAGA,UAAU,QAAb,oEAAqF,6CAArF,yBACA,gFAIA,uBAAGyB,GAAG,WAAWzB,UAAU,WAAU,6CAErC,0BAAM6B,YAAU,EAACC,aAAa,OAChC,kBAACC,EAAA,EAAD,CAAWN,GAAG,8BAA8BzB,UAAU,OAAOgC,WAAS,EAACC,QAAS,GAAIC,QAAQ,WAAWC,MAAM,gC,oDC3BpGC,MAnDf,WACE,OACE,yBAAKpC,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,iBAE1B,2BAAG,sFACH,yBAAKe,IAAKiB,IAAYf,IAAI,aAAaC,OAAO,MAAMC,MAAM,QAC1D,uBAAGxB,UAAU,YAAW,8BAAM,0BAAMA,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,qBAAT,aAA7B,OAAuF,0BAAML,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,gBAAT,kBAA9G,OAAwK,0BAAML,UAAU,SAAQ,kBAAC,WAAD,CAAMK,GAAG,kBAAT,wBAAhM,OAAkQ,0BAAML,UAAU,SAAQ,kBAAC,WAAD,CAAMK,GAAG,iBAAT,sBAA1R,OAAyV,0BAAML,UAAU,QAAO,kBAAC,WAAD,CAAMK,GAAG,qBAAT,eAExY,uBAAGoB,GAAG,WAAWzB,UAAU,WAAU,6CACrC,uBAAGsC,MAAM,QAAT,yNAEA,uBAAGb,GAAG,MAAMzB,UAAU,WAAU,kDAC9B,wBAAIA,UAAU,aACZ,+DACA,+CACA,sDAGJ,uBAAGyB,GAAG,QAAQzB,UAAU,WAAU,kEAElC,uBAAGA,UAAU,QAAb,gEAAiF,yDAAjF,qIAAuP,0CAAvP,MACE,uBAAGA,UAAU,QAAO,kBAAC,IAAD,CAAmBA,UAAU,UAAUuC,SAAS,OAAOpC,MAAOqC,KA7B7E,4HAiCP,uBAAGxC,UAAU,QAAb,SAA0B,0CAA1B,mDAA+F,8CAA/F,kEAAuL,kDAAvL,2BAA4O,0CAA5O,KAAmQ,2CAAnQ,SAA+R,wCAA/R,mEAAkX,0CAAlX,yBAA6Z,oEAA7Z,KACA,uBAAGsC,MAAM,QAAO,kBAAC,IAAD,CAAmBtC,UAAU,UAAUuC,SAAS,OAAOpC,MAAOqC,KAhCxE,8uBAoCN,uBAAGxC,UAAU,QAAb,qCAAsD,qCAAtD,kJACA,uBAAGA,UAAU,QAAO,kBAAC,IAAD,CAAmBA,UAAU,UAAUuC,SAAS,OAAOpC,MAAOqC,KAnC5E,sBAuCN,uBAAGf,GAAG,OAAOzB,UAAU,WAAU,sEAEjC,uBAAGA,UAAU,QAAb,kIAAmJ,0CAAnJ,mEAAwO,8CAAxO,0DAEA,yBAAKoB,IAAKqB,IAAQnB,IAAI,aAAaC,OAAO,MAAMC,MAAM,QACtD,uBAAGxB,UAAU,QAAb,iBAAkC,wCAAlC,mBAAqE,0CAArE,gTAGA,uBAAGyB,GAAG,WAAWzB,UAAU,WAAU,gDCyC9B0C,MA9Ff,WACE,OACE,yBAAK1C,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,YAE1B,4EACA,uBAAGL,UAAU,QAAb,mBCuFO0C,MA9Ff,WACE,OACE,yBAAK1C,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,YAE1B,0EACA,uBAAGL,UAAU,QAAb,mBCqFO2C,MA9Ff,WACE,OACE,yBAAK3C,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,YAE1B,0FACA,uBAAGL,UAAU,QAAb,mB,iBCuFO4C,MA9Ff,WACE,OACE,yBAAK5C,UAAU,OACb,4BAAQA,UAAU,cAChB,uBAAGA,UAAU,cAAa,8BAAM,kBAAC,UAAD,CAASC,OAAK,EAACC,gBAAgB,SAASC,MAAO,CAACC,MAAO,QAASC,GAAG,KAAnE,YAAN,YAE1B,0DACA,uBAAGL,UAAU,QAAb,gBAEA,yBAAKoB,IAAKyB,IAAKvB,IAAI,aAAaC,OAAO,MAAMC,MAAM,WCF3DsB,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,aAAD,KACM,yBAAKhD,UAAU,WACb,kBAAC,IAAD,CAAOC,OAAK,EAACgD,KAAK,IAAIC,UAAWnD,IACjC,kBAAC,IAAD,CAAOkD,KAAK,QAAQC,UAAW/B,IAC/B,kBAAC,IAAD,CAAO8B,KAAK,YAAYC,UAAWd,IACnC,kBAAC,IAAD,CAAOa,KAAK,SAASC,UAAWR,IAChC,kBAAC,IAAD,CAAOO,KAAK,QAAQC,UAAWC,IAC/B,kBAAC,IAAD,CAAOF,KAAK,SAASC,UAAWP,IAChC,kBAAC,IAAD,CAAOM,KAAK,OAAOC,UAAWN,OAIxCQ,SAASC,eAAe,SRqGpB,kBAAmBC,WACrBA,UAAUC,cAAcC,MACrBC,MAAK,SAAAC,GACJA,EAAaC,gBAEdC,OAAM,SAAAC,GACLC,QAAQD,MAAMA,EAAME,a,mBSzI5BlE,EAAOC,QAAU,IAA0B,wC,mBCA3CD,EAAOC,QAAU,IAA0B,oC,mBCA3CD,EAAOC,QAAU,IAA0B,iC","file":"static/js/main.634e5266.chunk.js","sourcesContent":["module.exports = __webpack_public_path__ + \"static/media/fakes012000.7fa05220.png\";","module.exports = __webpack_public_path__ + \"static/media/attempt_2_final.6bc3283f.png\";","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport './App.css';\n\nfunction App() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n\n        <p>\n          <code>&gt; Michaela Buchanan</code><br />\n          <code>==========================</code>\n        </p>\n        <div className=\"links\">\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          &nbsp;___  <span className=\"about\">Introduction</span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code><span className=\"about\">About Me</span>  ----|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"about\">Resume</span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"about\">Contact</span></code></p>\n\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code></p>\n\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          &nbsp;___  <span className=\"articles\"><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/gans\">GANs Sports Car Generation</NavLink></span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"articles\"><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/class\">Classification with Small Dataset</NavLink></span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code><span className=\"articles\">Articles</span>  ----|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"articles\"><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/coral\">Edge AI with RP & Google Coral</NavLink></span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"articles\"><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/gitpages\">Host React App on Github Pages</NavLink></span></code></p>\n<p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"articles\"><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/fire\">Firebase Database with Github Pages</NavLink></span></code></p>\n\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code></p>\n\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          &nbsp;___  <span className=\"cars\"><NavLink exact activeClassName=\"active\" style={{color: '#9933CC'}} to=\"/dsm\">1G DSM Street Build</NavLink></span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code><span className=\"cars\">Garage</span> &nbsp; ----|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"cars\">LS1 Camaro Drift Car</span></code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|</code></p>\n          <p><code>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n          |___  <span className=\"cars\">Yamaha R3 Track Bike</span></code></p>\n\n        </div>\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\nexport function register(config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl, config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl, config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' },\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import firebase from 'firebase'\n\nconst config = {\n    apiKey: \"AIzaSyCF6PIAdNEoLvOQS0untqthC5Mr8GGtEyM \",\n    authDomain: \"web-comments-f7751.firebaseapp.com\",\n    databaseURL: \"https://web-comments-f7751.firebaseio.com/\",\n    projectId: \"web-comments-f7751\",\n    storageBucket: \"web-comments-f7751.appspot.com\",\n    messagingSenderId: \"532954910646\"\n};\nfirebase.initializeApp(config);\nexport default firebase;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport { HashLink as Link } from 'react-router-hash-link';\nimport TextField from '@material-ui/core/TextField';\n\nimport result from \"../fakes012000.png\"\nimport result2 from \"../attempt_2_final.png\"\nimport comments from '../Comments'\n\nimport './Article.css';\n\nfunction Gans() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; GANs</code></p>\n\n        <p>Generating New Sports Cars using GANs</p>\n\n        <img src={result} alt=\"react logo\" height=\"350\" width=\"500\"/>\n\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gans#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gans#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#react\">Dataset</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#repo\">Progressive GANs</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#results\">Results</Link></span> -&gt;<span className=\"blue\"><Link to=\"gans#comments\"> Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p className=\"text\">This article is less of an instructive guide and more of a record of my attempts at generating sports car images using Generative Adversarial Networks (GANs). So far, while the results have been promising, they have not been of the level of quality that I am hoping to achieve. Some of the largest challenges I'm encountering include the small dataset easily available for \"sport\" or \"performance\" oriented car images as well as the inherent difficulty of training GANs. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n        <ul className=\"text list\">\n            <li>Familiarity with Deep Learning, specifically GANs</li>\n            <li>Have Tensorflow-GPU 1.15 installed</li>\n            <li>Access to GPUs (for faster training)</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Dataset and Image Augmentation</strong></p>\n        <p className=\"text\">Most of the dataset used for the first attempt of training has been scraped from the Yandex search engine. To do this, I followed the guide linked <a href=\"https://gist.github.com/imneonizer/23d2faa12833716e22830f807b082a58\" target=\"_blank\"><span className=\"blue\">here</span></a>. Search phrases such as \"sports car\", \"race car\", and \"drift car\" were used to compile these images. Then, various image augmentations were performed to increase the number of images available for training. The guide linked <a href=\"https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5\" target=\"_blank\"><span className=\"blue\">here</span></a> was used as a guide to performing these augmentations using <code>OpenCV</code>. I ended up using four different augmentations: vertical shift, horizontal shift, brightness changes, and zoom. This allowed me to increase my dataset size by four. However, after using this dataset, I am a bit concerned that some of these augmentations have an affect of the proportions of the cars generated by the network trained on these images so I need to further experiment with this strategy.</p>\n\n        <p className=\"text\">The dataset used for the second attempt at training was pulled from the Stanford Cars Dataset (linked <a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">here</span></a>). The purpose of this run of training is to see if much better results can be achieved with a larger, more consistent dataset. While this won't give me the \"sports car\" results I am looking for, this round of training will help guide my next steps to improving the model I am hoping to create, whether that be improving the dataset used or tweaking the model itself.</p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Progressive GANs</strong></p>\n        <p className=\"text\">I decided to use a progressive GANs model for this experiment. The model I used was developed by a team at NVIDIA and is linked <a href=\"https://github.com/tkarras/progressive_growing_of_gans\" target=\"_blank\"><span className=\"blue\">here</span></a>. The idea behind this variation of GANs is that the generator and discriminator are both trained starting at a very low resoluiton. The resolution is progressively increased throughout the training process until it eventually reaches the desired resolution (1024x1024 in this case). This makes it easier to achieve training stability, especially in situations like this one where there are a lot of fine details for the model to figure out.</p>\n\n        <p id=\"results\" className=\"subject\"><strong>Results</strong></p>\n\n        <p className=\"text\">The table below outlines some of the results of using this variety of GANs as well as the final product achieved.</p>\n        <table className=\"text tb\" width=\"80%\">\n        <tr>\n          <th>Attempt</th>\n          <th>Dataset</th>\n          <th>Dataset Size</th>\n          <th>Resolution</th>\n          <th>Run Time</th>\n          <th>GPUs Used</th>\n        </tr>\n        <tr>\n          <th>1</th>\n          <th>Custom web scraped</th>\n          <th>9524</th>\n          <th>1024x1024</th>\n          <th>2d 20h 51m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>2</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>8144</th>\n          <th>1024x1024</th>\n          <th>2d 20h 56m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>3</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>16,185</th>\n          <th>1024x1024</th>\n          <th>TBD</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        </table>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 1</strong> training this model:</p>\n        <img src={result} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">While the top middle picture was pretty good, the rest of the photos produced were not satisfactory. I began to wonder if the uncurated scraped dataset I was using was part of the problem. To test this, I decided to run another round of training using the same algorithm, this time using the Stanford car dataset linked in the table above. While this may not produce the \"sports\" car images I was hoping for, this test will allow me to determine if the image quality in the dataset was holding the model back.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 2</strong> training this model:</p>\n        <img src={result2} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">Once again, only one of the pictures produced could pass as a car. These results seem to indicate that the dataset quality was not the main problem the model was facing. In this second attempt, I only used the training image dataset provided by Stanford in order to keep the dataset size roughly equivalent to the one used in the first attempt. However, I now wanted to find out if increasing the dataset size would significantly increase the quality of the output produced. Therefore, for my third attempt, I added the testing dataset to the training dataset I already had to produce a new dataset which was about twice the size of those used previously.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 3</strong> training this model:</p>\n        <p>Image will be placed here once training finishes!</p>\n\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n        <form noValidate autoComplete=\"off\">\n      <TextField id=\"standard-multiline-flexible\" className=\"cbox\" multiline rowsMax={10} varient=\"outlined\" label=\"Type your comment here!\"/>\n    </form>\n\n      </header>\n    </div>\n  );\n}\nexport default Gans;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\n\nimport SyntaxHighlighter from 'react-syntax-highlighter';\nimport { dracula } from 'react-syntax-highlighter/dist/esm/styles/hljs';\nimport { dark } from 'react-syntax-highlighter/dist/esm/styles/prism';\nimport { HashLink as Link } from 'react-router-hash-link';\n\nimport react_logo from \"../react_logo.jpg\"\nimport git_sh from \"../git_sh.png\"\n\nimport './Article.css';\n\nconst code1 =  '1\\tcd your-username.github.io\\n2\\tnpx create-react-app your-react-project-className\\n3\\tnpm install gh-pages --save-dev'\n\nconst code2 = '1\\t{\\n2\\t\"name\": \"michaelabuchanan\",\\n3\\t\"homepage\": \"http://michaelabuchanan.github.io\",\\n4\\t\"version\": \"0.1.0\",\\n5\\t\"private\": true,\\n6\\t\"dependencies\": {\\n7\\t\"@material-ui/core\": \"^4.11.0\",\\n8\\t\"@testing-library/jest-dom\": \"^4.2.4\",\\n9\\t\"@testing-library/react\": \"^9.3.2\",\\n10\\t\"@testing-library/user-event\": \"^7.1.2\",\\n11\\t\"react\": \"^16.13.1\",\\n12\\t\"react-dom\": \"^16.13.1\",\\n13\\t\"react-router-dom\": \"^5.2.0\",\\n14\\t\"react-scripts\": \"3.4.3\",\\n15\\t\"react-syntax-highlighter\": \"^13.5.0\"\\n16\\t},\\n17\\t\"scripts\": {\\n18\\t\"start\": \"react-scripts start\",\\n19\\t\"predeploy\": \"npm run build\",\\n20\\t\"deploy\": \"gh-pages -d build\",\\n21\\t\"build\": \"react-scripts build\",\\n22\\t\"test\": \"react-scripts test\",\\n23\\t\"eject\": \"react-scripts eject\"\\n24\\t},\\n25\\t...'\n\nconst code3 = '1\\tnpm run deploy'\n\nfunction Gitpages() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; Git-Pages</code></p>\n\n        <p><strong>How to host your React app on Git-Pages for free </strong></p>\n        <img src={react_logo} alt=\"react logo\" height=\"350\" width=\"500\"/>\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gitpages#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gitpages#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gitpages#react\">Setup React Project</Link></span> -&gt; <span className=\"green\"><Link to=\"gitpages#repo\">Setup GitHub Repo</Link></span> -&gt; <span className=\"blue\"><Link to=\"gitpages#comments\">Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p class=\"text\">Are you looking for a free hosting service for your React application? If the application is for personal or development use, then GitPages may be a great alternative to more traditional website hosting services. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n          <ul className=\"text list\">\n            <li>Installed Node.js, NPM, and NPX</li>\n            <li>Installed React</li>\n            <li>Have a Github account</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Setting Up Your React Project</strong></p>\n\n        <p className=\"text\">First, create a GitHub repo for your website. Using the name <code>your-username.github.io</code> is recommended. Open a terminal and clone your new repo. Now run the following commands to create your React project and install <code>gh-pages</code>. </p>\n          <p className=\"text\"><SyntaxHighlighter className=\"codebox\" language=\"bash\" style={dracula}>\n          {code1}\n</SyntaxHighlighter></p>\n\n        <p className=\"text\">After <code>gh-pages</code> has successfully been installed, open the file <code>package.json</code> which should have been created in your directory by the above <code>create-react-app</code> command. Then, add the <code>homepage</code>, <code>predeploy</code>, and <code>deploy</code> lines shown below on lines 3, 19, and 20 to your file. For the <code>homepage</code> field, enter the url <code>http://your-git-username.github.io</code>.</p>\n        <p class=\"text\"><SyntaxHighlighter className=\"codebox\" language=\"json\" style={dracula}>\n          {code2}\n</SyntaxHighlighter></p>\n\n        <p className=\"text\">Now you should be able to run the <code>npm</code> command below to build the website content for GitHub to display. You should see \"Published\" after this command runs if it runs successfully.</p>\n        <p className=\"text\"><SyntaxHighlighter className=\"codebox\" language=\"json\" style={dracula}>\n          {code3}\n</SyntaxHighlighter></p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Setting Up Your Github Repository</strong></p>\n\n        <p className=\"text\">Your website is now build but you may still need to change some settings in your GitHub repo to make it viewable. If you go to <code>Settings</code> in your repo and scroll down, you should find a section titled <code>GitHub Pages</code>. It should look something like the screenshot below. </p>\n\n        <img src={git_sh} alt=\"react logo\" height=\"400\" width=\"700\"/>\n        <p className=\"text\">Make sure the <code>Branch</code> option has the <code>gh-pages</code> selected and leave the directory it looks for at / (root). After confirming these settings, you should be able to go to the link shown in these settings to see your published page. Note that it may take a few minutes for the site to be published so don't be alarmed if it doesn't appear immidiately.</p>\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n      </header>\n    </div>\n  );\n}\nexport default Gitpages;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport { HashLink as Link } from 'react-router-hash-link';\nimport TextField from '@material-ui/core/TextField';\n\nimport result from \"../fakes012000.png\"\nimport result2 from \"../attempt_2_final.png\"\nimport comments from '../Comments'\n\nimport './Article.css';\n\nfunction Classifier() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; GANs</code></p>\n\n        <p>Sports Car Classification with Small Datasets</p>\n        <p className=\"text\">Coming soon!</p>\n        {/*\n        <img src={result} alt=\"react logo\" height=\"350\" width=\"500\"/>\n\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gans#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gans#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#react\">Dataset</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#repo\">Progressive GANs</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#results\">Results</Link></span> -&gt;<span className=\"blue\"><Link to=\"gans#comments\"> Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p className=\"text\">This article is less of an instructive guide and more of a record of my attempts at generating sports car images using Generative Adversarial Networks (GANs). So far, while the results have been promising, they have not been of the level of quality that I am hoping to achieve. Some of the largest challenges I'm encountering include the small dataset easily available for \"sport\" or \"performance\" oriented car images as well as the inherent difficulty of training GANs. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n        <ul className=\"text list\">\n            <li>Familiarity with Deep Learning, specifically GANs</li>\n            <li>Have Tensorflow-GPU 1.15 installed</li>\n            <li>Access to GPUs (for faster training)</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Dataset and Image Augmentation</strong></p>\n        <p className=\"text\">Most of the dataset used for the first attempt of training has been scraped from the Yandex search engine. To do this, I followed the guide linked <a href=\"https://gist.github.com/imneonizer/23d2faa12833716e22830f807b082a58\" target=\"_blank\"><span className=\"blue\">here</span></a>. Search phrases such as \"sports car\", \"race car\", and \"drift car\" were used to compile these images. Then, various image augmentations were performed to increase the number of images available for training. The guide linked <a href=\"https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5\" target=\"_blank\"><span className=\"blue\">here</span></a> was used as a guide to performing these augmentations using <code>OpenCV</code>. I ended up using four different augmentations: vertical shift, horizontal shift, brightness changes, and zoom. This allowed me to increase my dataset size by four. However, after using this dataset, I am a bit concerned that some of these augmentations have an affect of the proportions of the cars generated by the network trained on these images so I need to further experiment with this strategy.</p>\n\n        <p className=\"text\">The dataset used for the second attempt at training was pulled from the Stanford Cars Dataset (linked <a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">here</span></a>). The purpose of this run of training is to see if much better results can be achieved with a larger, more consistent dataset. While this won't give me the \"sports car\" results I am looking for, this round of training will help guide my next steps to improving the model I am hoping to create, whether that be improving the dataset used or tweaking the model itself.</p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Progressive GANs</strong></p>\n        <p className=\"text\">I decided to use a progressive GANs model for this experiment. The model I used was developed by a team at NVIDIA and is linked <a href=\"https://github.com/tkarras/progressive_growing_of_gans\" target=\"_blank\"><span className=\"blue\">here</span></a>. The idea behind this variation of GANs is that the generator and discriminator are both trained starting at a very low resoluiton. The resolution is progressively increased throughout the training process until it eventually reaches the desired resolution (1024x1024 in this case). This makes it easier to achieve training stability, especially in situations like this one where there are a lot of fine details for the model to figure out.</p>\n\n        <p id=\"results\" className=\"subject\"><strong>Results</strong></p>\n\n        <p className=\"text\">The table below outlines some of the results of using this variety of GANs as well as the final product achieved.</p>\n        <table className=\"text tb\" width=\"80%\">\n        <tr>\n          <th>Attempt</th>\n          <th>Dataset</th>\n          <th>Dataset Size</th>\n          <th>Resolution</th>\n          <th>Run Time</th>\n          <th>GPUs Used</th>\n        </tr>\n        <tr>\n          <th>1</th>\n          <th>Custom web scraped</th>\n          <th>9524</th>\n          <th>1024x1024</th>\n          <th>2d 20h 51m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>2</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>8144</th>\n          <th>1024x1024</th>\n          <th>2d 20h 56m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>3</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>16,185</th>\n          <th>1024x1024</th>\n          <th>TBD</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        </table>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 1</strong> training this model:</p>\n        <img src={result} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">While the top middle picture was pretty good, the rest of the photos produced were not satisfactory. I began to wonder if the uncurated scraped dataset I was using was part of the problem. To test this, I decided to run another round of training using the same algorithm, this time using the Stanford car dataset linked in the table above. While this may not produce the \"sports\" car images I was hoping for, this test will allow me to determine if the image quality in the dataset was holding the model back.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 2</strong> training this model:</p>\n        <img src={result2} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">Once again, only one of the pictures produced could pass as a car. These results seem to indicate that the dataset quality was not the main problem the model was facing. In this second attempt, I only used the training image dataset provided by Stanford in order to keep the dataset size roughly equivalent to the one used in the first attempt. However, I now wanted to find out if increasing the dataset size would significantly increase the quality of the output produced. Therefore, for my third attempt, I added the testing dataset to the training dataset I already had to produce a new dataset which was about twice the size of those used previously.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 3</strong> training this model:</p>\n        <p>Image will be placed here once training finishes!</p>\n\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n        <form noValidate autoComplete=\"off\">\n      <TextField id=\"standard-multiline-flexible\" className=\"cbox\" multiline rowsMax={10} varient=\"outlined\" label=\"Type your comment here!\"/>\n    </form>\n    */}\n      </header>\n    </div>\n  );\n}\nexport default Classifier;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport { HashLink as Link } from 'react-router-hash-link';\nimport TextField from '@material-ui/core/TextField';\n\nimport result from \"../fakes012000.png\"\nimport result2 from \"../attempt_2_final.png\"\nimport comments from '../Comments'\n\nimport './Article.css';\n\nfunction Classifier() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; GANs</code></p>\n\n        <p>Using a Firebase Database with Github Pages</p>\n        <p className=\"text\">Coming soon!</p>\n        {/*\n        <img src={result} alt=\"react logo\" height=\"350\" width=\"500\"/>\n\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gans#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gans#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#react\">Dataset</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#repo\">Progressive GANs</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#results\">Results</Link></span> -&gt;<span className=\"blue\"><Link to=\"gans#comments\"> Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p className=\"text\">This article is less of an instructive guide and more of a record of my attempts at generating sports car images using Generative Adversarial Networks (GANs). So far, while the results have been promising, they have not been of the level of quality that I am hoping to achieve. Some of the largest challenges I'm encountering include the small dataset easily available for \"sport\" or \"performance\" oriented car images as well as the inherent difficulty of training GANs. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n        <ul className=\"text list\">\n            <li>Familiarity with Deep Learning, specifically GANs</li>\n            <li>Have Tensorflow-GPU 1.15 installed</li>\n            <li>Access to GPUs (for faster training)</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Dataset and Image Augmentation</strong></p>\n        <p className=\"text\">Most of the dataset used for the first attempt of training has been scraped from the Yandex search engine. To do this, I followed the guide linked <a href=\"https://gist.github.com/imneonizer/23d2faa12833716e22830f807b082a58\" target=\"_blank\"><span className=\"blue\">here</span></a>. Search phrases such as \"sports car\", \"race car\", and \"drift car\" were used to compile these images. Then, various image augmentations were performed to increase the number of images available for training. The guide linked <a href=\"https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5\" target=\"_blank\"><span className=\"blue\">here</span></a> was used as a guide to performing these augmentations using <code>OpenCV</code>. I ended up using four different augmentations: vertical shift, horizontal shift, brightness changes, and zoom. This allowed me to increase my dataset size by four. However, after using this dataset, I am a bit concerned that some of these augmentations have an affect of the proportions of the cars generated by the network trained on these images so I need to further experiment with this strategy.</p>\n\n        <p className=\"text\">The dataset used for the second attempt at training was pulled from the Stanford Cars Dataset (linked <a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">here</span></a>). The purpose of this run of training is to see if much better results can be achieved with a larger, more consistent dataset. While this won't give me the \"sports car\" results I am looking for, this round of training will help guide my next steps to improving the model I am hoping to create, whether that be improving the dataset used or tweaking the model itself.</p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Progressive GANs</strong></p>\n        <p className=\"text\">I decided to use a progressive GANs model for this experiment. The model I used was developed by a team at NVIDIA and is linked <a href=\"https://github.com/tkarras/progressive_growing_of_gans\" target=\"_blank\"><span className=\"blue\">here</span></a>. The idea behind this variation of GANs is that the generator and discriminator are both trained starting at a very low resoluiton. The resolution is progressively increased throughout the training process until it eventually reaches the desired resolution (1024x1024 in this case). This makes it easier to achieve training stability, especially in situations like this one where there are a lot of fine details for the model to figure out.</p>\n\n        <p id=\"results\" className=\"subject\"><strong>Results</strong></p>\n\n        <p className=\"text\">The table below outlines some of the results of using this variety of GANs as well as the final product achieved.</p>\n        <table className=\"text tb\" width=\"80%\">\n        <tr>\n          <th>Attempt</th>\n          <th>Dataset</th>\n          <th>Dataset Size</th>\n          <th>Resolution</th>\n          <th>Run Time</th>\n          <th>GPUs Used</th>\n        </tr>\n        <tr>\n          <th>1</th>\n          <th>Custom web scraped</th>\n          <th>9524</th>\n          <th>1024x1024</th>\n          <th>2d 20h 51m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>2</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>8144</th>\n          <th>1024x1024</th>\n          <th>2d 20h 56m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>3</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>16,185</th>\n          <th>1024x1024</th>\n          <th>TBD</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        </table>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 1</strong> training this model:</p>\n        <img src={result} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">While the top middle picture was pretty good, the rest of the photos produced were not satisfactory. I began to wonder if the uncurated scraped dataset I was using was part of the problem. To test this, I decided to run another round of training using the same algorithm, this time using the Stanford car dataset linked in the table above. While this may not produce the \"sports\" car images I was hoping for, this test will allow me to determine if the image quality in the dataset was holding the model back.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 2</strong> training this model:</p>\n        <img src={result2} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">Once again, only one of the pictures produced could pass as a car. These results seem to indicate that the dataset quality was not the main problem the model was facing. In this second attempt, I only used the training image dataset provided by Stanford in order to keep the dataset size roughly equivalent to the one used in the first attempt. However, I now wanted to find out if increasing the dataset size would significantly increase the quality of the output produced. Therefore, for my third attempt, I added the testing dataset to the training dataset I already had to produce a new dataset which was about twice the size of those used previously.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 3</strong> training this model:</p>\n        <p>Image will be placed here once training finishes!</p>\n\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n        <form noValidate autoComplete=\"off\">\n      <TextField id=\"standard-multiline-flexible\" className=\"cbox\" multiline rowsMax={10} varient=\"outlined\" label=\"Type your comment here!\"/>\n    </form>\n*/}\n      </header>\n    </div>\n  );\n}\nexport default Classifier;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport { HashLink as Link } from 'react-router-hash-link';\nimport TextField from '@material-ui/core/TextField';\n\nimport result from \"../fakes012000.png\"\n\nimport './Article.css';\n\nfunction Coral() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; GANs</code></p>\n\n        <p>Using a Raspberry Pi and Google Coral for AI Edge Inference</p>\n        <p className=\"text\">Coming soon!</p>\n        {/*\n        <img src={result} alt=\"react logo\" height=\"350\" width=\"500\"/>\n\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gans#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gans#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#react\">Dataset</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#repo\">Progressive GANs</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#results\">Results</Link></span> -&gt;<span className=\"blue\"><Link to=\"gans#comments\"> Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p className=\"text\">This article is less of an instructive guide and more of a record of my attempts at generating sports car images using Generative Adversarial Networks (GANs). So far, while the results have been promising, they have not been of the level of quality that I am hoping to achieve. Some of the largest challenges I'm encountering include the small dataset easily available for \"sport\" or \"performance\" oriented car images as well as the inherent difficulty of training GANs. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n        <ul className=\"text list\">\n            <li>Familiarity with Deep Learning, specifically GANs</li>\n            <li>Have Tensorflow-GPU 1.15 installed</li>\n            <li>Access to GPUs (for faster training)</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Dataset and Image Augmentation</strong></p>\n        <p className=\"text\">Most of the dataset used for the first attempt of training has been scraped from the Yandex search engine. To do this, I followed the guide linked <a href=\"https://gist.github.com/imneonizer/23d2faa12833716e22830f807b082a58\" target=\"_blank\"><span className=\"blue\">here</span></a>. Search phrases such as \"sports car\", \"race car\", and \"drift car\" were used to compile these images. Then, various image augmentations were performed to increase the number of images available for training. The guide linked <a href=\"https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5\" target=\"_blank\"><span className=\"blue\">here</span></a> was used as a guide to performing these augmentations using <code>OpenCV</code>. I ended up using four different augmentations: vertical shift, horizontal shift, brightness changes, and zoom. This allowed me to increase my dataset size by four. However, after using this dataset, I am a bit concerned that some of these augmentations have an affect of the proportions of the cars generated by the network trained on these images so I need to further experiment with this strategy.</p>\n\n        <p className=\"text\">The dataset used for the second attempt at training was pulled from the Stanford Cars Dataset (linked <a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">here</span></a>). The purpose of this run of training is to see if much better results can be achieved with a larger, more consistent dataset. While this won't give me the \"sports car\" results I am looking for, this round of training will help guide my next steps to improving the model I am hoping to create, whether that be improving the dataset used or tweaking the model itself.</p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Progressive GANs</strong></p>\n        <p className=\"text\">I decided to use a progressive GANs model for this experiment. The model I used was developed by a team at NVIDIA and is linked <a href=\"https://github.com/tkarras/progressive_growing_of_gans\" target=\"_blank\"><span className=\"blue\">here</span></a>. The idea behind this variation of GANs is that the generator and discriminator are both trained starting at a very low resoluiton. The resolution is progressively increased throughout the training process until it eventually reaches the desired resolution (1024x1024 in this case). This makes it easier to achieve training stability, especially in situations like this one where there are a lot of fine details for the model to figure out.</p>\n\n        <p id=\"results\" className=\"subject\"><strong>Results</strong></p>\n\n        <p className=\"text\">The table below outlines some of the results of using this variety of GANs as well as the final product achieved.</p>\n        <table className=\"text tb\" width=\"80%\">\n        <tr>\n          <th>Attempt</th>\n          <th>Dataset</th>\n          <th>Dataset Size</th>\n          <th>Resolution</th>\n          <th>Run Time</th>\n          <th>GPUs Used</th>\n        </tr>\n        <tr>\n          <th>1</th>\n          <th>Custom web scraped</th>\n          <th>9524</th>\n          <th>1024x1024</th>\n          <th>2d 20h 51m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>2</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>8144</th>\n          <th>1024x1024</th>\n          <th>2d 20h 56m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>3</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>16,185</th>\n          <th>1024x1024</th>\n          <th>TBD</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        </table>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 1</strong> training this model:</p>\n        <img src={result} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">While the top middle picture was pretty good, the rest of the photos produced were not satisfactory. I began to wonder if the uncurated scraped dataset I was using was part of the problem. To test this, I decided to run another round of training using the same algorithm, this time using the Stanford car dataset linked in the table above. While this may not produce the \"sports\" car images I was hoping for, this test will allow me to determine if the image quality in the dataset was holding the model back.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 2</strong> training this model:</p>\n        <img src={result2} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">Once again, only one of the pictures produced could pass as a car. These results seem to indicate that the dataset quality was not the main problem the model was facing. In this second attempt, I only used the training image dataset provided by Stanford in order to keep the dataset size roughly equivalent to the one used in the first attempt. However, I now wanted to find out if increasing the dataset size would significantly increase the quality of the output produced. Therefore, for my third attempt, I added the testing dataset to the training dataset I already had to produce a new dataset which was about twice the size of those used previously.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 3</strong> training this model:</p>\n        <p>Image will be placed here once training finishes!</p>\n\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n        <form noValidate autoComplete=\"off\">\n      <TextField id=\"standard-multiline-flexible\" className=\"cbox\" multiline rowsMax={10} varient=\"outlined\" label=\"Type your comment here!\"/>\n    </form>\n*/}\n      </header>\n    </div>\n  );\n}\nexport default Coral;\n","import React from 'react';\nimport { NavLink } from \"react-router-dom\";\nimport { HashLink as Link } from 'react-router-hash-link';\nimport TextField from '@material-ui/core/TextField';\n\nimport car from \"../pics/dsm.jpg\"\n\nimport './Article.css';\n\nfunction Dsm() {\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <p className=\"breadcrumb\"><code><NavLink exact activeClassName=\"active\" style={{color: 'cyan'}} to=\"/\">Articles</NavLink> &gt; GANs</code></p>\n\n        <p>1990 Mitsubishi Eclipse GSX</p>\n        <p className=\"text\">Coming soon!</p>\n\n        <img src={car} alt=\"react logo\" height=\"350\" width=\"500\"/>\n{/*\n        <p className=\"contents\"><code><span className=\"blue\"><Link to=\"gans#overview\">Overview</Link></span> -&gt; <span className=\"blue\"><Link to=\"gans#pre\">Prerequisites</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#react\">Dataset</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#repo\">Progressive GANs</Link></span> -&gt; <span className=\"green\"><Link to=\"gans#results\">Results</Link></span> -&gt;<span className=\"blue\"><Link to=\"gans#comments\"> Comments</Link></span></code></p>\n\n        <p id=\"overview\" className=\"subject\"><strong>Overview</strong></p>\n        <p className=\"text\">This article is less of an instructive guide and more of a record of my attempts at generating sports car images using Generative Adversarial Networks (GANs). So far, while the results have been promising, they have not been of the level of quality that I am hoping to achieve. Some of the largest challenges I'm encountering include the small dataset easily available for \"sport\" or \"performance\" oriented car images as well as the inherent difficulty of training GANs. </p>\n\n        <p id=\"pre\" className=\"subject\"><strong>Prerequisites</strong></p>\n        <ul className=\"text list\">\n            <li>Familiarity with Deep Learning, specifically GANs</li>\n            <li>Have Tensorflow-GPU 1.15 installed</li>\n            <li>Access to GPUs (for faster training)</li>\n          </ul>\n\n        <p id=\"react\" className=\"subject\"><strong>Dataset and Image Augmentation</strong></p>\n        <p className=\"text\">Most of the dataset used for the first attempt of training has been scraped from the Yandex search engine. To do this, I followed the guide linked <a href=\"https://gist.github.com/imneonizer/23d2faa12833716e22830f807b082a58\" target=\"_blank\"><span className=\"blue\">here</span></a>. Search phrases such as \"sports car\", \"race car\", and \"drift car\" were used to compile these images. Then, various image augmentations were performed to increase the number of images available for training. The guide linked <a href=\"https://towardsdatascience.com/complete-image-augmentation-in-opencv-31a6b02694f5\" target=\"_blank\"><span className=\"blue\">here</span></a> was used as a guide to performing these augmentations using <code>OpenCV</code>. I ended up using four different augmentations: vertical shift, horizontal shift, brightness changes, and zoom. This allowed me to increase my dataset size by four. However, after using this dataset, I am a bit concerned that some of these augmentations have an affect of the proportions of the cars generated by the network trained on these images so I need to further experiment with this strategy.</p>\n\n        <p className=\"text\">The dataset used for the second attempt at training was pulled from the Stanford Cars Dataset (linked <a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">here</span></a>). The purpose of this run of training is to see if much better results can be achieved with a larger, more consistent dataset. While this won't give me the \"sports car\" results I am looking for, this round of training will help guide my next steps to improving the model I am hoping to create, whether that be improving the dataset used or tweaking the model itself.</p>\n\n        <p id=\"repo\" className=\"subject\"><strong>Progressive GANs</strong></p>\n        <p className=\"text\">I decided to use a progressive GANs model for this experiment. The model I used was developed by a team at NVIDIA and is linked <a href=\"https://github.com/tkarras/progressive_growing_of_gans\" target=\"_blank\"><span className=\"blue\">here</span></a>. The idea behind this variation of GANs is that the generator and discriminator are both trained starting at a very low resoluiton. The resolution is progressively increased throughout the training process until it eventually reaches the desired resolution (1024x1024 in this case). This makes it easier to achieve training stability, especially in situations like this one where there are a lot of fine details for the model to figure out.</p>\n\n        <p id=\"results\" className=\"subject\"><strong>Results</strong></p>\n\n        <p className=\"text\">The table below outlines some of the results of using this variety of GANs as well as the final product achieved.</p>\n        <table className=\"text tb\" width=\"80%\">\n        <tr>\n          <th>Attempt</th>\n          <th>Dataset</th>\n          <th>Dataset Size</th>\n          <th>Resolution</th>\n          <th>Run Time</th>\n          <th>GPUs Used</th>\n        </tr>\n        <tr>\n          <th>1</th>\n          <th>Custom web scraped</th>\n          <th>9524</th>\n          <th>1024x1024</th>\n          <th>2d 20h 51m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>2</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>8144</th>\n          <th>1024x1024</th>\n          <th>2d 20h 56m</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        <tr>\n          <th>3</th>\n          <th>Stanford dataset (<a href=\"http://ai.stanford.edu/~jkrause/cars/car_dataset.html\" target=\"_blank\"><span className=\"blue\">link</span></a>)</th>\n          <th>16,185</th>\n          <th>1024x1024</th>\n          <th>TBD</th>\n          <th>4xNVIDIA V100</th>\n        </tr>\n        </table>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 1</strong> training this model:</p>\n        <img src={result} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">While the top middle picture was pretty good, the rest of the photos produced were not satisfactory. I began to wonder if the uncurated scraped dataset I was using was part of the problem. To test this, I decided to run another round of training using the same algorithm, this time using the Stanford car dataset linked in the table above. While this may not produce the \"sports\" car images I was hoping for, this test will allow me to determine if the image quality in the dataset was holding the model back.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 2</strong> training this model:</p>\n        <img src={result2} alt=\"react logo\" height=\"600\" width=\"900\"/>\n\n        <p className=\"text\">Once again, only one of the pictures produced could pass as a car. These results seem to indicate that the dataset quality was not the main problem the model was facing. In this second attempt, I only used the training image dataset provided by Stanford in order to keep the dataset size roughly equivalent to the one used in the first attempt. However, I now wanted to find out if increasing the dataset size would significantly increase the quality of the output produced. Therefore, for my third attempt, I added the testing dataset to the training dataset I already had to produce a new dataset which was about twice the size of those used previously.<br /></p>\n\n        <p className=\"text\">Here are the fake images that were produced on the last round of <strong>attempt 3</strong> training this model:</p>\n        <p>Image will be placed here once training finishes!</p>\n\n\n\n        <p id=\"comments\" className=\"subject\"><strong>Comments</strong></p>\n\n        <form noValidate autoComplete=\"off\">\n      <TextField id=\"standard-multiline-flexible\" className=\"cbox\" multiline rowsMax={10} varient=\"outlined\" label=\"Type your comment here!\"/>\n    </form>\n*/}\n      </header>\n    </div>\n  );\n}\nexport default Dsm;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport { Route, HashRouter } from 'react-router-dom';\nimport * as serviceWorker from './serviceWorker';\n\nimport firebase from './firebase.js';\n\nimport Gans from './pages/Gans'\nimport Gitpages from './pages/Gitpages'\nimport Classifier from './pages/Classifier'\nimport Firebase from './pages/Firebase'\nimport Coral from './pages/Coral'\nimport Dsm from './pages/Dsm'\n\nReactDOM.render(\n  <React.StrictMode>\n    <HashRouter>\n          <div className=\"content\">\n            <Route exact path=\"/\" component={App}/>\n            <Route path=\"/gans\" component={Gans}/>\n            <Route path=\"/gitpages\" component={Gitpages}/>\n            <Route path=\"/class\" component={Classifier}/>\n            <Route path=\"/fire\" component={Firebase}/>\n            <Route path=\"/coral\" component={Coral}/>\n            <Route path=\"/dsm\" component={Dsm}/>\n          </div>\n        </HashRouter>\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.unregister();\n","module.exports = __webpack_public_path__ + \"static/media/react_logo.7bdceb15.jpg\";","module.exports = __webpack_public_path__ + \"static/media/git_sh.43f253c1.png\";","module.exports = __webpack_public_path__ + \"static/media/dsm.68bb1f06.jpg\";"],"sourceRoot":""}